{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating Different Decoding Strategies with Hugging Face Transformers\n",
    "In this notebook, we will explore various decoding strategies for text generation using a small encoder-decoder model from Hugging Face's Transformers library. We'll apply these strategies to two different datasets:\n",
    "\n",
    "- Translation task where deterministic strategies are expected to perform better.\n",
    "- Summarization task where stochastic strategies might yield more diverse and informative outputs.\n",
    "\n",
    "The decoding strategies we'll test include:\n",
    "\n",
    "1. Greedy Search\n",
    "2. Beam Search\n",
    "3. Temperature Sampling\n",
    "4. Top-k Sampling\n",
    "5. Top-p (Nucleus) Sampling\n",
    "\n",
    "We'll define a custom ```generate_text``` function to apply these strategies and evaluate their performance using appropriate metrics for each dataset.\n",
    "\n",
    "Here are some useful links you might want to check:\n",
    "- [Auto Classes](https://huggingface.co/docs/transformers/model_doc/auto)\n",
    "- [transformers\\AutoModel](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel)\n",
    "- [transformers\\AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer)\n",
    "- [Google's T5](https://arxiv.org/pdf/1910.10683)\n",
    "- [T5-small](https://huggingface.co/google-t5/t5-small)\n",
    "- [Flan-T5-small](https://huggingface.co/google/flan-t5-small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets sacrebleu rouge_score evaluate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Pre-trained Model and Tokenizer\n",
    "\n",
    "We'll use the ```flan-T5-small``` model, which is a small encoder-decoder model suitable for both story_gen and summarization tasks. This model is based on Google's ```t5-small``` model, but fine-tuned on more than 1000 additional tasks covering also more languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-small'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_text, strategy='greedy', max_length=50, **kwargs):\n",
    "    \"\"\"Generates text based on the specified decoding strategy.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input text to be processed by the model.\n",
    "        strategy (str, optional): The decoding strategy to use. Defaults to 'greedy'.\n",
    "            Options include:\n",
    "            - 'greedy': Greedy search decoding.\n",
    "            - 'beam': Beam search decoding.\n",
    "            - 'temperature': Temperature sampling.\n",
    "            - 'top-k': Top-k sampling.\n",
    "            - 'top-p': Top-p (nucleus) sampling.\n",
    "            - 'contrastive': Contrastive search decoding.\n",
    "        max_length (int, optional): The maximum length of the generated text. Defaults to 50.\n",
    "        **kwargs: Additional keyword arguments specific to the decoding strategy.\n",
    "\n",
    "    Keyword Args:\n",
    "        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n",
    "            Used when `strategy='beam'`.\n",
    "        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n",
    "            Used when `strategy='temperature'`.\n",
    "        top_k (int, optional): The number of highest probability vocabulary tokens to keep for top-k filtering. Defaults to 50.\n",
    "            Used when `strategy='top-k'` or when `strategy='contrastive'`.\n",
    "        top_p (float, optional): Cumulative probability for nucleus sampling. Defaults to 0.95.\n",
    "            Used when `strategy='top-p'`.\n",
    "        penalty_alpha (float, optional): Contrastive search penalty factor. Defaults to 0.6.\n",
    "            Used when `strategy='contrastive'`.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text based on the decoding strategy.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an unknown decoding strategy is specified.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if strategy == 'greedy':\n",
    "            output_ids = model.generate(input_ids, max_length=max_length)\n",
    "        elif strategy == 'beam':\n",
    "            num_beams = kwargs.get('num_beams', 5)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, num_beams=num_beams, early_stopping=True\n",
    "            )\n",
    "        elif strategy == 'temperature':\n",
    "            temperature = kwargs.get('temperature', 1.0)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, temperature=temperature\n",
    "            )\n",
    "        elif strategy == 'top-k':\n",
    "            top_k = kwargs.get('top_k', 50)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, top_k=top_k\n",
    "            )\n",
    "        elif strategy == 'top-p':\n",
    "            top_p = kwargs.get('top_p', 0.95)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, top_p=top_p\n",
    "            )\n",
    "        elif strategy == 'contrastive':\n",
    "            penalty_alpha = kwargs.get('penalty_alpha', 0.6)\n",
    "            top_k = kwargs.get('top_k', 4)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, penalty_alpha=penalty_alpha, top_k=top_k\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Unknown strategy: {}\".format(strategy))\n",
    "\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return output_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case 1: Neural Machine Translation\n",
    "The WMT16 English-German dataset is a collection of parallel sentences in English and German used for machine translation tasks. It is part of the Conference on Machine story_gen (WMT) shared tasks, which are benchmarks for evaluating machine story_gen systems. The dataset contains professionally translated sentences and covers a variety of topics, making it ideal for training and evaluating story_gen models.\n",
    "\n",
    "We'll use a subset (1% of the test split) of the dataset for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_nmt = load_dataset('wmt16', 'de-en', split='test[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      "Durch den von Obama beworbenen Deal um das iranische Atomprogramm hat sich die Beziehung der beiden weiter verschlechtert.\n",
      "\n",
      "Target Text:\n",
      "The relationship between the two has further deteriorated because of the deal that Obama negotiated on Iran's atomic programme, .\n"
     ]
    }
   ],
   "source": [
    "# Randomly select a sample from the dataset\n",
    "sample = random.choice(dataset_nmt['translation'])\n",
    "input_text = sample['de']\n",
    "target_text = sample['en']\n",
    "\n",
    "print(\"Input Text:\")\n",
    "print(input_text)\n",
    "print(\"\\nTarget Text:\")\n",
    "print(target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to preprocess the dataset to prepare it for input into the T5 model. The T5 model expects input in a specific format, including a task prefix. This is because the T5 model is a multi-purpose model, being able to perform several task, so we need to tell it what it must do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_translation(examples: list[dict]) -> dict:\n",
    "    \"\"\"Preprocesses a single example for the translation task.\n",
    "\n",
    "    Args:\n",
    "        examples list[dict]: A list of dictionaries containing 'de' and 'en' keys with English and German sentences.\n",
    "\n",
    "    Returns:\n",
    "        dict[list]: A dictionary of lists with added 'src_texts' and 'tgt_texts' keys for model input and target.\n",
    "\n",
    "    The function:\n",
    "    - Adds the task prefix 'translate German to English: ' to the German sentence.\n",
    "    - Stores the result in 'src_texts'.\n",
    "    - Copies the English sentence to 'tgt_texts'.\n",
    "    \"\"\"\n",
    "    # Empty dictionary\n",
    "    texts = {}\n",
    "    \n",
    "    # T5 expects a \"translate German to English: \" prefix\n",
    "    texts['src_texts'] = ['translate German to English: ' + ex['de'] for ex in examples]\n",
    "    texts['tgt_texts'] = [ex['en'] for ex in examples]\n",
    "    return texts\n",
    "\n",
    "dataset_nmt_preproc = preprocess_translation(dataset_nmt[\"translation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can generate a translation using the t5 model. From a random dataset, we are going to create translations using the different implemented strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate German to English: Die Polizei in Gautier, wo Prentiss starb, sagte, er habe keine Geschichte der Gewalt oder eine kriminelle Vorgeschichte.\n",
      "Original translation: \n",
      "Police in Gautier, where Prentiss died, said he had no history of violence or criminal record.\n",
      "Greedy Search: \n",
      "The police in Gautier, where Prentiss was murdered, said he had no history of violence or a critical history.\n",
      "Beam Search: \n",
      "The police in Gautier, where Prentiss said he had no history of violence or a critical history.\n",
      "Temperature Sampling: \n",
      "Police in Gautier, where Prentiss starred, told him no history of violence or a critical historical record.\n",
      "Top-k Sampling: \n",
      "The police in Gautier, where Prentiss was shot, stated that he was not involved with any history of violence or a Greek-themed historical war.\n",
      "Top-p Sampling: \n",
      "The police in Gautier, where Prentiss’s younger sister, said he did not have history of violence or a critical past.\n"
     ]
    }
   ],
   "source": [
    "# Obtain source and target texts\n",
    "random_index = random.randint(0, len(dataset_nmt_preproc['src_texts']))\n",
    "random_src_sentence = dataset_nmt_preproc['src_texts'][random_index]\n",
    "random_tgt_sentence = dataset_nmt_preproc['tgt_texts'][random_index]\n",
    "\n",
    "# Obtain the translated sentence\n",
    "translation_greedy= generate_text(random_src_sentence, strategy='greedy')\n",
    "translation_beam_search = generate_text(random_src_sentence, strategy='beam', num_beams=5)\n",
    "translation_temperature = generate_text(random_src_sentence, strategy='temperature', temperature=0.7)\n",
    "translation_top_k = generate_text(random_src_sentence, strategy='top-k', top_k=50)\n",
    "translation_top_p = generate_text(random_src_sentence, strategy='top-p', top_p=0.95)\n",
    "# translation_contrastive = generate_text(random_src_sentence, strategy='contrastive', top_k=4, penalty_alpha=0.6)\n",
    "\n",
    "print(random_src_sentence)\n",
    "print(f\"Original translation: \\n{random_tgt_sentence}\")\n",
    "print(f\"Greedy Search: \\n{translation_greedy}\")\n",
    "print(f\"Beam Search: \\n{translation_beam_search}\")\n",
    "print(f\"Temperature Sampling: \\n{translation_temperature}\")\n",
    "print(f\"Top-k Sampling: \\n{translation_top_k}\")\n",
    "print(f\"Top-p Sampling: \\n{translation_top_p}\")\n",
    "# print(f\"Contrastive Search: \\n{translation_contrastive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do you see something different between the deterministic and the stochastic strategies? Try different random sentences.\n",
    ">>> Deterministic strategies like greedy and beam search produce consistent and predictable translations by always picking the most likely next word, which makes them stable but sometimes rigid or slightly inaccurate. Stochastic strategies like temperature, top-k, and top-p sampling introduce randomness, generating more diverse outputs that can be creative but often include semantic errors or irrelevant details. In short, deterministic methods prioritize accuracy and consistency, while stochastic methods favor variety at the cost of reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric: BLEU Score\n",
    "#### What is BLEU Score?\n",
    "The BLEU (Bilingual Evaluation Understudy) score is a metric for evaluating the quality of text that has been machine-translated from one language to another. It compares a candidate translation to one or more reference translations and calculates a score based on the overlap of n-grams (contiguous sequences of words).\n",
    "\n",
    "BLEU-4: Considers up to 4-gram matches between the candidate and reference translations. It provides a balance between precision (matching words) and fluency (maintaining the structure of the language). Using BLEU-4 allows us to capture not just individual word matches (unigrams) but also phrases of up to four words. This makes the evaluation more sensitive to the quality of the translation in terms of both accuracy and fluency.\n",
    "\n",
    "We'll use the ```sacrebleu``` implementation for a standardized BLEU score calculation (which is BLEU-4). You can check the details [here](https://aclanthology.org/W14-3346.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 8.15kB [00:00, 2.39MB/s]\n"
     ]
    }
   ],
   "source": [
    "bleu_metric = evaluate.load('sacrebleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating strategy: greedy\n",
      "BLEU-4 score for greedy: 20.99\n",
      "Evaluating strategy: beam\n",
      "BLEU-4 score for beam: 21.14\n",
      "Evaluating strategy: temperature\n",
      "BLEU-4 score for temperature: 8.91\n",
      "Evaluating strategy: top-k\n",
      "BLEU-4 score for top-k: 11.37\n",
      "Evaluating strategy: top-p\n",
      "BLEU-4 score for top-p: 12.55\n"
     ]
    }
   ],
   "source": [
    "# strategies = ['greedy', 'beam', 'temperature', 'top-k', 'top-p', 'contrastive']\n",
    "strategies = ['greedy', 'beam', 'temperature', 'top-k', 'top-p']\n",
    "results = {}\n",
    "\n",
    "hyperparameters = {\n",
    "    'num_beams': 5,\n",
    "    'temperature': 1.0,\n",
    "    'top_k': 50,\n",
    "    'top_p': 0.95,\n",
    "    'penalty_alpha': 0.6\n",
    "}\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"Evaluating strategy: {strategy}\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for src_text, tgt_text in zip(dataset_nmt_preproc[\"src_texts\"], dataset_nmt_preproc[\"tgt_texts\"]): \n",
    "        pred = generate_text(src_text, strategy=strategy, **hyperparameters)\n",
    "        predictions.append(pred)\n",
    "        references.append([tgt_text])  # SacreBLEU expects a list of references\n",
    "\n",
    "    # Compute BLEU-4 score\n",
    "    bleu = bleu_metric.compute(predictions=predictions, references=references, smooth_method='exp')\n",
    "    results[strategy] = bleu['score']\n",
    "    print(f\"BLEU-4 score for {strategy}: {bleu['score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seeing the above translations and the BLEU score of the different strategies, which strategy would you choose for this use case?\n",
    ">>> Based on the translations and BLEU scores, I would choose a deterministic strategy, preferably beam search, because it achieves the highest BLEU score (21.14) and produces more accurate and consistent translations. Stochastic strategies like temperature, top-k, and top-p generate more varied outputs but have much lower BLEU scores, indicating they are less reliable for precise translation in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case 2: Story generation\n",
    "\n",
    "The ```WritingPrompts``` dataset is a collection of imaginative prompts and corresponding stories from the Reddit community. It contains over 300,000 stories written in response to various prompts, making it suitable for training and evaluating models on creative text generation tasks.\n",
    "\n",
    "We'll use a subset of the dataset for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 232360/232360 [00:02<00:00, 100723.23 examples/s]\n",
      "Generating sample_length_10_to_292 split: 100%|██████████| 1000/1000 [00:00<00:00, 235993.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_st_gen = load_dataset('llm-aes/writing-prompts', split='train[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text:\n",
      " In the far future where most forms of armed combat have become obsolete , you are a member of the last remaining ground squad in existence . One day , you are deployed on a mission where all other options have failed . Detail this deployment .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Randomly select a sample from the dataset\n",
    "sample = random.choice(dataset_st_gen['prompt'])\n",
    "\n",
    "print(\"Sample Text:\")\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to tell the t5 model to generate text after the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_story_generation(examples: list[dict]) -> dict:\n",
    "    \"\"\"Preprocesses a single example for the story generation task.\n",
    "\n",
    "    Args:\n",
    "        examples (list[dict]): A list of dictionaries containing 'prompt' key.\n",
    "\n",
    "    Returns:\n",
    "        dict[list]: A dictionary of list with added 'src_texts' for model input.\n",
    "\n",
    "    The function:\n",
    "    - Adds the task prefix 'Write a story based on: ' to the prompt.\n",
    "    \"\"\"\n",
    "    # Empty dictionary\n",
    "    texts = {}\n",
    "    \n",
    "    # T5 expects a task prefix\n",
    "    texts['src_texts'] = ['Write a story based on: ' + ex['prompt'] for ex in examples]\n",
    "    return texts\n",
    "\n",
    "dataset_st_gen_preproc = preprocess_story_generation(dataset_st_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have no original story to compare how good the model generates a story, you should compare the different decoding strategies by looking at some random stories: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a story based on:  If you murder someone , your jail sentence is as long as their remaining life would have been .\n",
      "\n",
      "Greedy Search: \n",
      "a man who murders a man , he is a smuggler , and he is a smuggler . If you murder someone , your sentence is as long as their remaining life would have been .\n",
      "Beam Search: \n",
      "If you murder someone , your sentence is as long as their remaining life would have been . If you murder someone , your sentence is as long as their remaining life would have been .\n",
      "Temperature Sampling: \n",
      "a man in his 80's , an indian , a teen , and a teen . If he was a cop , the sentence of incarceration was as long as his life . The , if , penalty was as long as their last life .\n",
      "Top-k Sampling: \n",
      "If you are convicted of murdering someone while they were a little child , some might not have convicted you of murdering them . A few weeks when you murder another child , you may have a lot of problems with the sentence - but, by time , you may have convicted the child at all. Sometimes , all of those problems have been solved - but you may be convicted of a lesser offense . The sentence is about as long as one night of incarceration , once they were there - but it remains the same in jail .\n",
      "Top-p Sampling: \n",
      "If you kill someone and you feel sorry for them , you can be the killer . If they did , they can still be the killer . If you are caught , you'll have to get a fine or jail sentence. If the victim is not in the room and he can't find his way , your sentence may take weeks .\n"
     ]
    }
   ],
   "source": [
    "# Obtain source and target texts\n",
    "random_index = random.randint(0, len(dataset_st_gen_preproc['src_texts']))\n",
    "random_src_sentence = dataset_st_gen_preproc['src_texts'][random_index]\n",
    "\n",
    "# Obtain the translated sentence\n",
    "story_gen_greedy= generate_text(random_src_sentence, max_length=300, strategy='greedy')\n",
    "story_gen_beam_search = generate_text(random_src_sentence, max_length=300, strategy='beam', num_beams=5)\n",
    "story_gen_temperature = generate_text(random_src_sentence, max_length=300, strategy='temperature', temperature=0.7)\n",
    "story_gen_top_k = generate_text(random_src_sentence, max_length=300, strategy='top-k', top_k=50)\n",
    "story_gen_top_p = generate_text(random_src_sentence, max_length=300, strategy='top-p', top_p=0.95)\n",
    "# story_gen_contrastive = generate_text(random_src_sentence, max_length=300, strategy='contrastive', top_k=4, penalty_alpha=0.6)\n",
    "\n",
    "print(random_src_sentence)\n",
    "print(f\"Greedy Search: \\n{story_gen_greedy}\")\n",
    "print(f\"Beam Search: \\n{story_gen_beam_search}\")\n",
    "print(f\"Temperature Sampling: \\n{story_gen_temperature}\")\n",
    "print(f\"Top-k Sampling: \\n{story_gen_top_k}\")\n",
    "print(f\"Top-p Sampling: \\n{story_gen_top_p}\")\n",
    "# print(f\"Contrastive Search: \\n{story_gen_contrastive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seeing the above generated stories of the different strategies, which strategy would you choose for this use case?\n",
    ">>> For this use case, I would choose greedy search, since it produces the clearest and most coherent story, directly following the prompt without repeating or drifting off-topic. Beam search is overly repetitive, while stochastic strategies like temperature, top-k, and top-p create confusing or inconsistent stories that don’t reflect the idea effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis\n",
    "\n",
    "Try different hyperparameter values for the decoding strategies, try to optimize the BLEU score for the Neural Machine Translation case and generate better stories in the second use case.\n",
    "\n",
    "- Which optimal configuration have you found for use case 1? Which are your conclusions based on your analysis?\n",
    ">>> The optimal configuration was beam search with a moderate beam size (3 to 5). This gave the highest BLEU-4 score (21.14 aprox.) and produced translations that were both accurate and fluent. Greedy search was close in performance but slightly less flexible, while stochastic strategies produced more errors and lower BLEU scores. The conclusion is that deterministic strategies work best for translation tasks where precision and consistency are important.\n",
    "\n",
    "- Which optimal configuration have you found for use case 2? Which are your conclusions bsaed on your analysis?\n",
    ">>> The optimal configuration was greedy decoding. It generated logically consistent and coherent stories directly following the prompt. Stochastic methods, like temperature, top-k, top-p, produced longer but more confusing or unrealistic narratives, and beam search became a little repetitive. So, for tasks requiring clear, focused output, greedy decoding can outperform stochastic strategies, even if the text is shorter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
